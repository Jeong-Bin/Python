Requests
    - 웹 페이지(html) 읽어오기
    - 주어진 url을 통해 받아온 html에 원하는 정보가 있을 때 사용
    - 빠르다
    - 동적 웹 페이지 X
    - res.raise_for_status() 를 이용해 접속에 문제가 없는지 확인해야 함


Selenium
    - 웹 페이지 자동화
    - 로그인이나 어떤 결과에 대한 필터링 등 특정 동작을 해야할 때 사용
    - 느리다
    - 동적 웹 페이지 O
    - 자신의 크롬 버전에 맞는 chromedriver.exe가 반드시 있어야 함
    - find_element(s)_by_id             id 로 찾기
    - find_element(s)_by_class_name     class name 으로 찾기
    - find_element(s)_by_link_text      링크 text 로 찾기
    - find_element(s)_by_xpath          xpath로 찾기
    - click()                           클릭
    - send_keys()                       글자 입력
    - clear()                           원래 있던 글자 지우기


BeautifulSoup
    - 웹 스크래핑
    - 위의 Requests 혹은 Selenium 으로 가져운 웹페이지를 데이터로 추출
    - find                              조건에 맞는 첫 번째 element 찾기
    - find_all                          조건에 맞는 모든 element를 리스트로 찾기
    - find_next_sibling(s)              다음 형제 찾기
    - find_previous_sibling(s)          이전 형제 찾기
    - soup["href"]                      속성 가져오기
    - soup.get_text                     텍스트 가져오기


이미지 다운로드
    with open("파일명", "wb") as f:
        f.write(res.content)


CSV
    import csv 
        f = open(filename. "w", encoding="utf-8-sig", newline"")


Headless Chrome
    - 브라우저를 띄우지 않고 동작함
    - 때로는 User-Agent 정의 필요
    - 59 버전부터 사용 가능 (최신 버전(20년8월이후)이면 가능)


* 웹 스크래핑 주의사항!
    - 무분별한 웹 크롤링 / 스크래핑은 대상 서버에 부하를 줌
        -> 계정 / IP 차단

    - 데이터 사용 주의
        -> 이미지, 텍스트 등 데이터 무단 활용 시 저작권 등 침해요소. 법적 제재

    - robot.txt
        (google.com/robot.txt)
        -> 이게 Disallow로 표시된 데이터는 가져가지 말라는 뜻
        -> 법적 효력은 없고 대상 사이트의 권고
        -> Allow는 괜찮음